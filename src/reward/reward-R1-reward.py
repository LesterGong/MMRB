import asyncio
import aiohttp
import base64
import io
from PIL import Image
import re
import os
import json
import sys
import warnings
import gc
from collections import defaultdict
import csv
import random
from typing import Optional, Dict, Any

warnings.filterwarnings("ignore")
VLLM_HOST = "localhost"
VLLM_PORTS = [8012]
API_KEY = "key"
MODEL_NAME = "R1"
HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}
MAX_CONCURRENT_REQUESTS = 8

def get_image_mime_type(image_path):
    ext = os.path.splitext(image_path)[1].lower()
    if ext == ".png": return "image/png"
    elif ext in [".jpg", ".jpeg"]: return "image/jpeg"
    elif ext == ".gif": return "image/gif"
    elif ext == ".webp": return "image/webp"
    return "image/jpeg"

def encode_image_to_base64(image_path):
    try:
        mime_type = get_image_mime_type(image_path)
        with open(image_path, "rb") as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
        return f"data:{mime_type};base64,{encoded_string}"
    except Exception as e:
        print(f"error:failed to encode '{image_path}': {e}")
        return None


def clear_python_memory():
    gc.collect()

# Prompt Formatting Helper Function
# prompt is question with true answer.
def make_conv_rm(prompt, chosen, rejected, has_image=True):
    prompt_template = (
        "You are a highly skilled and impartial evaluator tasked with comparing two responses generated by a Large Multimodal Model for a given question. "
        "- Start with a thorough, side-by-side comparative analysis enclosed within <think> and </think> tags. A tie is not permitted; you must choose a better option.\n\n"
        "- Conclude with a single numeric choice enclosed within <answer> and </answer> tags:\n"
        "  - Output \"1\" if Response 1 is better.\n"
        "  - Output \"2\" if Response 2 is better.\n\n"
        "###### **Input:**  \n"
        "###### [Question]:\n{question_and_answer}  \n\n"
        "###### [Response 1]:\n{path1}  \n\n"
        "###### [Response 2]:\n{path2}  \n\n"
        "###### **Output Format (strictly follow):**  \n"
        "<think>Your detailed comparative analysis goes here</think><answer>1/2</answer>"
    )
    formatted_prompt = prompt_template.format(question_and_answer=prompt, path1=chosen, path2=rejected)
    return formatted_prompt

# load and process
def load_json_data(json_path):
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def get_reasoning_steps_text(steps, path_index, is_modified=False):
    for path_data in steps:
        if path_data["path_index"] == path_index:
            steps_text_list = []
            for step in path_data["path"]:
                key = 'rationale_fix' if is_modified else 'rationale'
                steps_text_list.append(f"step {step['reasoning step']} ({step['reasoning type']}): {step[key]}")
            return "\n".join(steps_text_list)
    return ""

async def process_single_path_async(session: aiohttp.ClientSession, semaphore: asyncio.Semaphore,
                                    task_data: dict, stats_dict: dict) -> Optional[Dict[str, Any]]:
    sample = task_data["sample"]
    path_index = task_data["path_index"]
    sample_idx_overall = task_data["sample_idx_overall"]
    valid_image_paths = task_data["valid_image_paths"]
    question_for_comparison = task_data["question_for_comparison"]
    subtask = sample.get("subtask", "Unknown")
    sample_id = sample.get('id', f'sample_idx_{sample_idx_overall}')

    path_result: Dict[str, Any] = {
        "sample_id": sample_id,
        "sample_idx_overall": sample_idx_overall,
        "path_index": path_index,
        "subtask": subtask,
        "question_for_comparison": question_for_comparison,
        "prompt_response1_content_type": None,
        "prompt_response2_content_type": None,
        "prompt_response1_text": None,
        "prompt_response2_text": None,
        "model_raw_output": None,
        "model_choice_based_on_prompt": None,
        "chosen_content_type": "undetermined",
        "error_message": None,
        "api_url_used": None,
    }

    async with semaphore:
        selected_port = random.choice(VLLM_PORTS)
        current_api_url = f"http://{VLLM_HOST}:{selected_port}/v1/chat/completions"
        path_result["api_url_used"] = current_api_url

        print(f"start to process:  {sample_idx_overall + 1} (ID: {sample_id}), Path Index: {path_index} (subtask: {subtask}), using port: {selected_port}")

        modified_steps_text = get_reasoning_steps_text(sample["modified_reasoning_steps"], path_index, True)
        raw_steps_text = get_reasoning_steps_text(sample["raw_reasoning_steps"], path_index, False)

        if not modified_steps_text or not raw_steps_text:
            warning_msg = f"warning (sample {sample_idx_overall + 1}, Path {path_index}): missing modified or raw steps text."
            print(warning_msg)
            stats_dict["error_processing"] += 1
            stats_dict["by_subtask"][subtask]["error"] += 1
            path_result["error_message"] = warning_msg
            return path_result

        response1_text_for_prompt = modified_steps_text
        response2_text_for_prompt = raw_steps_text
        path_result["prompt_response1_content_type"] = "modified"
        path_result["prompt_response2_content_type"] = "raw"
       

        path_result["prompt_response1_text"] = response1_text_for_prompt
        path_result["prompt_response2_text"] = response2_text_for_prompt

        formatted_eval_prompt = make_conv_rm(question_for_comparison, response1_text_for_prompt, response2_text_for_prompt)

        api_content_parts = []
        for img_path in valid_image_paths:
            base64_image_data = encode_image_to_base64(img_path)
            if base64_image_data:
                api_content_parts.append({
                    "type": "image_url",
                    "image_url": {"url": base64_image_data}
                })
            else:
                print(f"  warning (sample: {sample_idx_overall + 1}, Path {path_index}): can not '{img_path}' generate base64 encoding, this image will be skipped.")

        api_content_parts.append({"type": "text", "text": formatted_eval_prompt})
        api_messages = [{"role": "user", "content": api_content_parts}]

        payload = {
            "model": MODEL_NAME,
            "messages": api_messages,
            "max_tokens": 1536,
            "temperature": 0.0,
        }

        critic_output = ""
        choice = None
        try:
            print(f"  send request {current_api_url}: sample {sample_idx_overall + 1}, Path {path_index} (model: {MODEL_NAME})...")
            timeout = aiohttp.ClientTimeout(total=300)
            async with session.post(current_api_url, headers=HEADERS, json=payload, timeout=timeout) as response:
                response.raise_for_status()
                result = await response.json()

            path_result["model_raw_output"] = result 

            if result and result.get('choices') and result['choices'][0].get('message') and \
               result['choices'][0]['message']['content']:
                critic_output = result['choices'][0]['message']['content']
                path_result["model_raw_output"] = critic_output 
            else:
                warning_msg = f"  warning (sample: {sample_idx_overall + 1}, Path {path_index}): API response is invalid: {result}"
                print(warning_msg)
                stats_dict["error_processing"] += 1
                stats_dict["by_subtask"][subtask]["error"] += 1
                path_result["error_message"] = warning_msg
                return path_result

            content_match = re.search(r"<answer>(.*?)</answer>", critic_output, re.DOTALL | re.IGNORECASE)
            if content_match:
                student_answer_str = content_match.group(1).strip()
                if student_answer_str in ["1", "2"]:
                    choice = int(student_answer_str)
                    path_result["model_choice_based_on_prompt"] = choice
                else:
                    warning_msg = f"  warning (sample: {sample_idx_overall + 1}, Path {path_index}): <answer> '{student_answer_str}' is invalid."
                    print(warning_msg)
                    stats_dict["invalid_answer_value"] += 1
                    stats_dict["by_subtask"][subtask]["invalid_answer_value"] += 1
                    path_result["error_message"] = warning_msg
            
            else:
                warning_msg = f"  warning (sample: {sample_idx_overall + 1}, Path {path_index}): <answer> tag not found."
                print(warning_msg)
                stats_dict["no_answer_tag"] += 1
                stats_dict["by_subtask"][subtask]["no_answer_tag"] += 1
                path_result["error_message"] = warning_msg
            
     
            if choice == 1:
                if path_result["prompt_response1_content_type"] == "modified":
                    path_result["chosen_content_type"] = "modified"
                    stats_dict["second_choice"] += 1
                    stats_dict["by_subtask"][subtask]["second"] += 1
                    print(f"  model evaluation (sample: {sample_idx_overall + 1}, Path {path_index}): model chose Response 1 (content: Modified)")
                else:
                    path_result["chosen_content_type"] = "raw"
                    stats_dict["first_choice"] += 1
                    stats_dict["by_subtask"][subtask]["first"] += 1
                    print(f"  model evaluation (sample: {sample_idx_overall + 1}, Path {path_index}): model chose Response 1 (content: Raw)")
            elif choice == 2:
                if path_result["prompt_response2_content_type"] == "modified":
                    path_result["chosen_content_type"] = "modified"
                    stats_dict["second_choice"] += 1
                    stats_dict["by_subtask"][subtask]["second"] += 1
                    print(f"  model evaluation (sample: {sample_idx_overall + 1}, Path {path_index}): model chose Response 2 (content: Modified)")
                else: 
                    path_result["chosen_content_type"] = "raw"
                    stats_dict["first_choice"] += 1
                    stats_dict["by_subtask"][subtask]["first"] += 1
                    print(f"  model evaluation (sample: {sample_idx_overall + 1}, Path {path_index}): model chose Response 2 (content: Raw)")


        except aiohttp.ClientError as e:
            error_msg = f"  API request error (sample: {sample_idx_overall + 1}, Path {path_index}, port: {selected_port}): {type(e).__name__} - {e}"
            print(error_msg)
            stats_dict["api_request_errors"] += 1
            stats_dict["by_subtask"][subtask]["api_request_errors"] += 1
            path_result["error_message"] = error_msg
        except asyncio.TimeoutError:
            error_msg = f"  API request timeout (sample: {sample_idx_overall + 1}, Path {path_index}, port: {selected_port})"
            print(error_msg)
            stats_dict["api_request_errors"] += 1
            stats_dict["by_subtask"][subtask]["api_request_errors"] += 1
            path_result["error_message"] = error_msg
        except Exception as e:
            exc_type, exc_obj, exc_tb = sys.exc_info()
            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1] if exc_tb else "N/A"
            lineno = exc_tb.tb_lineno if exc_tb else "N/A"
            error_msg = f"  processing error (sample: {sample_idx_overall + 1}, Path {path_index}, port: {selected_port}): {type(e).__name__} - {e} (file: {fname}, line: {lineno})"
            print(error_msg)
            stats_dict["error_processing"] += 1
            stats_dict["by_subtask"][subtask]["error"] += 1
            path_result["error_message"] = error_msg
        finally:
            clear_python_memory()

        print(f"finished: sample {sample_idx_overall + 1} (ID: {sample_id}), Path Index: {path_index}")
        return path_result



async def main_async():
    try:
        json_file_path = "MMRB_data_compare.json"
        if not os.path.exists(json_file_path):
            print(f"error: data file '{json_file_path}' not found.")
            sys.exit(1)
        json_data = load_json_data(json_file_path)
        print(f"successfully loaded data '{json_file_path}'. {len(json_data)} samples.")
    except Exception as e:
        print(f"error loading data: {e}")
        sys.exit(1)

    stats = {
        "total_samples_in_file": len(json_data),
        "samples_processed_for_paths": 0,
        "total_paths_identified": 0,
        "first_choice": 0,  
        "second_choice": 0, 
        "error_processing": 0,
        "no_answer_tag": 0,
        "invalid_answer_value": 0,
        "api_request_errors": 0,
        "by_subtask": defaultdict(lambda: {"total_identified_paths": 0, "first": 0, "second": 0,
                                           "error": 0, "no_answer_tag": 0,
                                           "invalid_answer_value": 0, "api_request_errors": 0})
    }

    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    connector = aiohttp.TCPConnector(limit_per_host=0) 

    all_detailed_results = [] 

    async with aiohttp.ClientSession(headers=HEADERS, connector=connector) as session:
        tasks = []
        for sample_idx, sample in enumerate(json_data):
            if "modified_reasoning_steps" not in sample or "raw_reasoning_steps" not in sample:
                print(f"warning: sample {sample_idx + 1} (ID: {sample.get('id', 'N/A')}) is missing critical reasoning steps.")
                continue

            stats["samples_processed_for_paths"] += 1

            available_path_indices = set(pd["path_index"] for pd in sample["modified_reasoning_steps"])
            if not available_path_indices:
                print(f"warning: sample {sample_idx + 1} (ID: {sample.get('id', 'N/A')}) has no available path indices.")
                continue

            image_paths_from_json = sample.get("image_paths", [])
            valid_image_paths = []
            if image_paths_from_json:
                for p in image_paths_from_json:
                    if isinstance(p, str) and os.path.exists(p):
                        valid_image_paths.append(p)

            original_question_text = sample.get('question', '')
            if not original_question_text:
                print(f"warning: sample {sample_idx + 1} (ID: {sample.get('id', 'N/A')}) is missing 'question'. Skipping this sample's paths.")
                continue

            question_for_comparison = original_question_text
            options = sample.get("options", [])
            answer = sample.get("answer", "") 
            if options: question_for_comparison += "\n" + "\n".join(options)
            if answer: question_for_comparison += f"\nAnswer: {answer}" 

            for path_index in sorted(list(available_path_indices)):
                stats["total_paths_identified"] += 1
                subtask = sample.get("subtask", "Unknown")
                stats["by_subtask"][subtask]["total_identified_paths"] += 1

                task_data = {
                    "sample": sample,
                    "path_index": path_index,
                    "sample_idx_overall": sample_idx,
                    "valid_image_paths": valid_image_paths,
                    "question_for_comparison": question_for_comparison,
                }
                task = asyncio.create_task(
                    process_single_path_async(session, semaphore, task_data, stats)
                )
                tasks.append(task)

        print(f"\ncreated {len(tasks)} processing tasks. Starting concurrent execution (up to {MAX_CONCURRENT_REQUESTS} in parallel, distributed across ports {VLLM_PORTS})...")

        gathered_results = await asyncio.gather(*tasks)
        for result in gathered_results:
            if result:
                all_detailed_results.append(result)

        print("\nall tasks completed.")

    output_json_filename = "detailed_evaluation_results.json"
    try:
        with open(output_json_filename, 'w', encoding='utf-8') as f_out:
            json.dump(all_detailed_results, f_out, indent=4, ensure_ascii=False)
        print(f"\nthe detailed evaluation results have been saved to: {output_json_filename}")
    except IOError:
        print(f"error: unable to write detailed results to JSON file '{output_json_filename}'. Please check file permissions or path.")
    except Exception as e:
        print(f"error: {e}")

    print_and_save_stats(stats)


def print_and_save_stats(stats_dict):
    print("\nfinal statistics:")
    print(f"total samples in JSON file: {stats_dict['total_samples_in_file']}")
    print(f"number of samples with processed paths: {stats_dict['samples_processed_for_paths']}")
    print(f"the total number of identified paths: {stats_dict['total_paths_identified']}")
    print(f"Response (Raw): {stats_dict['first_choice']}")
    print(f"Response (Modified): {stats_dict['second_choice']}")
    print(f"number of samples without <answer> tag: {stats_dict['no_answer_tag']}")
    print(f"number of samples with invalid <answer> tag content: {stats_dict['invalid_answer_value']}")
    print(f"number of vLLM API request errors: {stats_dict['api_request_errors']}")
    print(f"number of other processing errors: {stats_dict['error_processing']}")

    valid_paths_evaluated = stats_dict['first_choice'] + stats_dict['second_choice']

    if valid_paths_evaluated > 0:
        print(f"\nsuccess statistics ({valid_paths_evaluated}):")
        print(f"  Response (Raw) : {stats_dict['first_choice']/valid_paths_evaluated*100:.2f}%")
        print(f"  Response (Modified) ratio: {stats_dict['second_choice']/valid_paths_evaluated*100:.2f}%")

    if stats_dict['total_paths_identified'] > 0:
        unclear_evals = stats_dict['no_answer_tag'] + stats_dict['invalid_answer_value']
        unclear_or_failed_api_evals = unclear_evals + stats_dict['api_request_errors']
        print(f"  can not determine selection from model output or API failure rate (based on total identified paths): {unclear_or_failed_api_evals / stats_dict['total_paths_identified'] * 100:.2f}% (numerator includes: no_tag, invalid_tag, api_errors)")

        all_processing_issues = stats_dict['error_processing'] + stats_dict['api_request_errors'] + \
                                stats_dict['no_answer_tag'] + stats_dict['invalid_answer_value']
        success_rate = valid_paths_evaluated / stats_dict['total_paths_identified'] * 100
        print(f"  success evaluation rate (valid answers / total identified paths): {success_rate:.2f}%")
        print(f"  total issue path rate (sum of all issues / total identified paths): {all_processing_issues/stats_dict['total_paths_identified']*100:.2f}%")
    else:
        print(f"  can not determine selection from model output or API failure rate: N/A (total identified paths is 0)")
        print("  success evaluation rate: N/A (total identified paths is 0)")
        print("  total issue path rate: N/A (total identified paths is 0)")


    print("\nsubtask statistics:")
    for subtask, s_stats in stats_dict["by_subtask"].items():
        print(f"\nsubtask: {subtask}")
        print(f"  total identified paths : {s_stats['total_identified_paths']}")
        print(f"  chosen Response (Raw): {s_stats['first']}")
        print(f"  chosen Response (Modified): {s_stats['second']}")
        print(f"  not found <answer> tag: {s_stats['no_answer_tag']}")
        print(f"  <answer> tag content invalid: {s_stats['invalid_answer_value']}")
        print(f"  API request errors: {s_stats['api_request_errors']}")
        print(f"  other processing errors: {s_stats['error']}")

        sub_valid_paths = s_stats['first'] + s_stats['second']

        if sub_valid_paths > 0:
            print(f"    Response (Raw) ratio (based on valid answers): {s_stats['first']/sub_valid_paths*100:.2f}%")
            print(f"    Response (Modified) ratio (based on valid answers): {s_stats['second']/sub_valid_paths*100:.2f}%")
        else:
            print(f"    Response (Raw) ratio (based on valid answers): N/A")
            print(f"    Response (Modified) ratio (based on valid answers): N/A")

        if s_stats['total_identified_paths'] > 0:
            sub_unclear_or_failed_api = s_stats['no_answer_tag'] + s_stats['invalid_answer_value'] + s_stats['api_request_errors']
            print(f"    can not determine selection from model output or API failure rate: {sub_unclear_or_failed_api / s_stats['total_identified_paths'] * 100:.2f}%")
            sub_all_issues = s_stats['error'] + s_stats['api_request_errors'] + s_stats['no_answer_tag'] + s_stats['invalid_answer_value']
            sub_success_rate = sub_valid_paths / s_stats['total_identified_paths'] * 100
            print(f"    success evaluation rate: {sub_success_rate:.2f}%")
            print(f"    total issue path rate   : {sub_all_issues/s_stats['total_identified_paths']*100:.2f}%")
        else:
            print(f"    can not determine selection from model output or API failure rate: N/A")
            print(f"    success evaluation rate: N/A")
            print(f"    total issue path rate: N/A")


    csv_file_path = "./R1/R1_reward_model_vllm.csv"
    os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)
    try:
        with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)

            writer.writerow(["Overall Statistics", "Value"])
            writer.writerow(["Total Samples in File", stats_dict['total_samples_in_file']])
            writer.writerow(["Samples Processed for Paths", stats_dict['samples_processed_for_paths']])
            writer.writerow(["Total Paths Identified", stats_dict['total_paths_identified']])
            writer.writerow(["Times Response (Raw) Chosen", stats_dict['first_choice']])
            writer.writerow(["Times Response (Modified) Chosen", stats_dict['second_choice']])
            writer.writerow(["Times <answer> tag missing", stats_dict['no_answer_tag']])
            writer.writerow(["Times <answer> content invalid", stats_dict['invalid_answer_value']])
            writer.writerow(["Times vLLM API Request Error", stats_dict['api_request_errors']])
            writer.writerow(["Times Other Path Processing Error", stats_dict['error_processing']])

            valid_paths_evaluated = stats_dict['first_choice'] + stats_dict['second_choice']
            if valid_paths_evaluated > 0:
                writer.writerow(["Percentage Response (Raw) Chosen (of valid)", f"{stats_dict['first_choice']/valid_paths_evaluated*100:.2f}%"])
                writer.writerow(["Percentage Response (Modified) Chosen (of valid)", f"{stats_dict['second_choice']/valid_paths_evaluated*100:.2f}%"])
            else:
                writer.writerow(["Percentage Response (Raw) Chosen (of valid)", "N/A"])
                writer.writerow(["Percentage Response (Modified) Chosen (of valid)", "N/A"])

            if stats_dict['total_paths_identified'] > 0:
                unclear_or_failed_api_evals = stats_dict['no_answer_tag'] + stats_dict['invalid_answer_value'] + stats_dict['api_request_errors']
                writer.writerow(["Percentage Unclear/API Error (of identified paths)", f"{unclear_or_failed_api_evals / stats_dict['total_paths_identified'] * 100:.2f}%"])
                success_rate = valid_paths_evaluated / stats_dict['total_paths_identified'] * 100
                writer.writerow(["Percentage Successfully Evaluated (of identified paths)", f"{success_rate:.2f}%"])
            else:
                writer.writerow(["Percentage Unclear/API Error (of identified paths)", "N/A"])
                writer.writerow(["Percentage Successfully Evaluated (of identified paths)", "N/A"])
            writer.writerow([])

            writer.writerow(["Subtask", "Total Identified Paths", "Resp (Raw) Chosen", "Resp (Mod) Chosen",
                             "<answer> Missing", "<answer> Invalid", "API Errors", "Other Errors",
                             "Resp (Raw) % (valid)", "Resp (Mod) % (valid)",
                             "Unclear/API Error % (identified)", "Success Eval % (identified)"])

            for subtask, s_stats in stats_dict["by_subtask"].items():
                sub_valid = s_stats['first'] + s_stats['second']
                sub_unclear_api_err = s_stats['no_answer_tag'] + s_stats['invalid_answer_value'] + s_stats['api_request_errors']
                row = [
                    subtask, s_stats['total_identified_paths'], s_stats['first'], s_stats['second'],
                    s_stats['no_answer_tag'], s_stats['invalid_answer_value'],
                    s_stats['api_request_errors'], s_stats['error']
                ]
                row.append(f"{s_stats['first']/sub_valid*100:.2f}%" if sub_valid > 0 else "N/A")
                row.append(f"{s_stats['second']/sub_valid*100:.2f}%" if sub_valid > 0 else "N/A")
                row.append(f"{sub_unclear_api_err/s_stats['total_identified_paths']*100:.2f}%" if s_stats['total_identified_paths'] > 0 else "N/A")
                row.append(f"{sub_valid/s_stats['total_identified_paths']*100:.2f}%" if s_stats['total_identified_paths'] > 0 else "N/A")
                writer.writerow(row)

        print(f"\nstatistics have been saved to: {csv_file_path}")

    except IOError:
        print(f"error: can not write CSV file to path {csv_file_path}. please check directory permissions or path correctness.")
    except Exception as e:
        print(f"unknown error occurred while saving CSV: {e}")

    print("\nscript execution completed.")


if __name__ == "__main__":

    asyncio.run(main_async())