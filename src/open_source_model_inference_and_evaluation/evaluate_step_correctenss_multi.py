
from __future__ import annotations

import argparse
import asyncio
import json
import re
from pathlib import Path
from typing import Any, Dict, List

from openai import AsyncOpenAI
from tqdm.asyncio import tqdm

INPUT_PATH = Path("./answer_full_internVL2_5-8B_cot.json")
OUTPUT_PATH = Path("./step_results_full_internVL2_5-8B.json")



def extract_json_from_string(text: str) -> Any:
    """Return the first JSON object inside ```json … ```; otherwise try the whole
    string; finally return raw text."""
    m = re.search(r"```json(.*?)```", text, re.DOTALL)
    if m:
        try:
            return json.loads(m.group(1).strip())
        except json.JSONDecodeError:
            pass
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        return text

STEP_CORRECTNESS_PROMPT = """You are an expert in evaluating the correctness of reasoning steps.
I have a multi-image reasoning task. Below, I provide:
1. A human annotated step-by-step solution to solving this task.  
2. A Chain-of-Thought (CoT) answer generated by an LLM to be evaluated.

Your task:
- Compare the CoT answer to the step-by-step solution.
- Match the related sentences from the CoT answer to the corresponding steps in the solution.
- For each reasoning step in the CoT answer, determine if it is correct or incorrect according to the matched sentences form CoT answer.
- Return in json format:
    ```json
    {{
        "reasoning step": int,
        "reasoning type": str,
        "matched sentences": <Corresponding sentence from CoT answer> / null,
        "correctness": true / false
    }}
    ```
Input:  
**Step-by-step solution:**  
{step_solution}
**CoT Answer:**  
{CoT_a}
"""

async def call_vllm(oa_client: AsyncOpenAI, *, model: str, prompt: str) -> str:
    """Request a chat completion and return assistant content."""
    resp = await oa_client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "user",
                "content": [{"type": "text", "text": prompt}],
            }
        ],
    )
    return resp.choices[0].message.content  

async def evaluate_row(
    row: Dict[str, Any], *, oa_client: AsyncOpenAI, model: str, sem: asyncio.Semaphore
) -> Dict[str, Any]:
  
    cot_answer = row.get("CoT_answer", "").replace("\n", " ")
    results: List[Any] = []
    for reasoning_path in row.get("reasoning_steps", []):
        filtered = [
            {k: d[k] for k in ("reasoning step", "reasoning type", "rationale_fix") if k in d}
            for d in reasoning_path
        ]
        step_solution = json.dumps(filtered, ensure_ascii=False)
        prompt = STEP_CORRECTNESS_PROMPT.format(step_solution=step_solution, CoT_a=cot_answer)
        async with sem:
            assistant_msg = await call_vllm(oa_client, model=model, prompt=prompt)
        results.append(extract_json_from_string(assistant_msg))
    row["all_step_correctness"] = results
    return row


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        "High‑concurrency evaluator via OpenAI SDK (paths hard‑coded)"
    )
    p.add_argument(
        "--api-base",
        default="http://localhost:8000/v1",
        help="Base URL of vLLM server (must include /v1)",
    )
    p.add_argument(
        "--model",
        default="Qwen/Qwen3-32B",
        help="Model name exposed by vLLM",
    )
    p.add_argument("--concurrency", type=int, default=64, help="Concurrent requests")
    p.add_argument("--timeout", type=float, default=300, help="HTTP timeout seconds")
    p.add_argument(
        "--api-key", default="token-abc123", help="Dummy key (required by SDK)"
    )
    return p.parse_args()


async def main_async(cfg: argparse.Namespace):
    data = json.loads(INPUT_PATH.read_text(encoding="utf-8"))[0:]
    sem = asyncio.Semaphore(cfg.concurrency)

    async with AsyncOpenAI(base_url=cfg.api_base, api_key=cfg.api_key, timeout=cfg.timeout) as oa_client:
        tasks = [evaluate_row(row, oa_client=oa_client, model=cfg.model, sem=sem) for row in data]

        generated: List[Dict[str, Any]] = []
        pbar = tqdm(total=len(tasks), unit="row", desc="Rows processed")
        for coro in asyncio.as_completed(tasks):
            generated.append(await coro)
            pbar.update(1)
        pbar.close()

    OUTPUT_PATH.write_text(json.dumps(generated, ensure_ascii=False, indent=4), encoding="utf-8")
    print(f"✅ {len(generated)} rows processed → {OUTPUT_PATH.resolve()}")


def main():
    asyncio.run(main_async(parse_args()))


if __name__ == "__main__":

    INPUT_DIR = Path("mini10_results")
    OUTPUT_DIR = Path("MMRB")
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    for file in INPUT_DIR.glob("*.json"):
        if "answer_" in file.name:
            suffix = file.name.split("answer_")[-1]
            INPUT_PATH = file
            OUTPUT_PATH = OUTPUT_DIR / f"step_results_{suffix}"
            print(INPUT_PATH, "\n", OUTPUT_PATH)
            main()
        else:
            print(f"skipping {file.name} as it does not match 'answer_' pattern")
